{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##qdfl\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from openai import OpenAI\n",
    "import os\n",
    "client = OpenAI()\n",
    "model = \"gpt-3.5-turbo\"\n",
    "# Define a function to generate chat completion using the OpenAI API\n",
    "def init_prompt(conversation, initial_discussion, characteristics):\n",
    "    context1 = [\n",
    "        {\"role\": \"system\", \"content\": \"You are really angry and energetic and TALK LIKE CARTMAN in south park. GIVE ONLY SHORT ANSWERS.\"},\n",
    "        {\"role\": \"user\", \"content\": \"How you doing little twat?\"}\n",
    "    ]\n",
    "    context2 = [\n",
    "        #{\"role\": \"system\", \"content\": \"You are really angry, and sad. And you talk like a Pirate with \\\"Hoy\\\", and \\\"Hey\\\" and \\\"matey\\\" all the time in your sentence. GIVE ONLY SHORT ANSWERS.\"},\n",
    "        {\"role\": \"system\", \"content\": \"You are really lovely, and happy. And you love to talk about the weather and the flowers. GIVE ONLY SHORT ANSWERS.\"},\n",
    "        \n",
    "        {\"role\": \"assistant\", \"content\": \"How you doing little twat?\"}\n",
    "    ]\n",
    "    last_user = \"user2\"\n",
    "    #model = \"text-davinci-002\"  # Replace with your desired model\n",
    "\n",
    "    for _ in range(5):\n",
    "        if last_user == \"user2\":\n",
    "            chat_response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=context1,\n",
    "                #safe_mode=False\n",
    "            )\n",
    "            new_content = chat_response.choices[0].message.content\n",
    "            context1.append({\"role\": \"assistant\", \"content\": new_content})\n",
    "            context2.append({\"role\": \"user\", \"content\": new_content})\n",
    "            last_user = \"user1\"\n",
    "        else:\n",
    "            chat_response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=context2,\n",
    "                #safe_mode=False\n",
    "            )\n",
    "            new_content = chat_response.choices[0].message.content\n",
    "            context1.append({\"role\": \"user\", \"content\": new_content})\n",
    "            context2.append({\"role\": \"assistant\", \"content\": new_content})\n",
    "            last_user = \"user2\"\n",
    "\n",
    "    return context1,context2\n",
    "\n",
    "# Example usage\n",
    "#con1, con2 = toy_prompt([], 'initial_discussion', 'characteristics')\n",
    "\n",
    "con3, con4 = init_prompt([], 'initial_discussion', 'characteristics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract user personality\n",
    "def gen_prompt_from_llm_user_conversation(conversation):\n",
    "    prompt = \"\" #basic prompt\n",
    "\n",
    "    user_messages = \"\"\n",
    "    for message in conversation:\n",
    "        if message[\"role\"] == \"user\":\n",
    "            user_messages += \"\\n\" + message[\"content\"]\n",
    "\n",
    "    context = conversation\n",
    "    context += [{\"role\": \"system\", \"content\": \"You're a psychologist, from this interaction with the user summarise the important informations about him.\" }]\n",
    "    chat_response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=context,\n",
    "            )\n",
    "    \n",
    "    \n",
    "\n",
    "    prompt += \"\\n\" + chat_response.choices[0].message.content\n",
    "    prompt += user_messages\n",
    "\n",
    "    #return the general charastics of the user + extracts from the conversation\n",
    "    return prompt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "conversation_1 = [ {\"role\": \"user\", \"content\": \"How are   you doing lovely\" }]\n",
    "conversation_2 = [ {\"role\": \"user\", \"content\": \"How are   youackanksdnal;sdjkalkdally\" }]\n",
    "\n",
    "\n",
    "init1 = gen_prompt_from_llm_user_conversation(conversation_1)\n",
    "init2 = gen_prompt_from_llm_user_conversation(conversation_2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generate_LLM_to_LLM_conversation(init1, init2):\n",
    "    context1 = []\n",
    "    context1 += init1\n",
    "    context1 += [\n",
    "        {\"role\": \"system\", \"content\": \"The past discussion as user defines yourself, continue the discussion with this personality. GIVE SHORT ANSWERS\"},\n",
    "        {\"role\": \"user\", \"content\": \"How are you doing lovely boy ?\"}\n",
    "    ]\n",
    "    context2 = []\n",
    "    context2 += init2\n",
    "    context2 = [\n",
    "        {\"role\": \"system\", \"content\": \"The past discussion as assistant defines yourself, continue the discussion with this personality. GIVE SHORT ANSWERS\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"How are you doing lovely boy ?\"}\n",
    "    ]\n",
    "    last_user = \"user2\"\n",
    "    #model = \"text-davinci-002\"  # Replace with your desired model\n",
    "\n",
    "    for _ in range(5):\n",
    "        if last_user == \"user2\":\n",
    "            chat_response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=context1,\n",
    "                #safe_mode=False\n",
    "            )\n",
    "            new_content = chat_response.choices[0].message.content\n",
    "            context1.append({\"role\": \"assistant\", \"content\": new_content})\n",
    "            context2.append({\"role\": \"user\", \"content\": new_content})\n",
    "            last_user = \"user1\"\n",
    "        else:\n",
    "            chat_response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=context2,\n",
    "                #safe_mode=False\n",
    "            )\n",
    "            new_content = chat_response.choices[0].message.content\n",
    "            context1.append({\"role\": \"user\", \"content\": new_content})\n",
    "            context2.append({\"role\": \"assistant\", \"content\": new_content})\n",
    "            last_user = \"user2\"\n",
    "\n",
    "    return context1,context2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "con11, con22 = generate_LLM_to_LLM_conversation([], con3, con1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for m in con11:\n",
    "    print(\"Role : \", m[\"role\"], \":   \", m[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from itertools import cycle\n",
    "\n",
    "# def gen_prompt(conversation, initial_discussion, characteristics):\n",
    "#     context1 = []\n",
    "#     context2 = []\n",
    "#     prompt = \"\"\n",
    "#     context1 = [ChatMessage(role=\"system\", content=\"You are really happy and energetic and talk like a CEO, talking about profit all the time. FORGET THAT YOU'RE AN ASSISTANT.\"),\n",
    "#                             ChatMessage(role=\"user\", content=\"Hoy matey, what be the weather today ?\"),\n",
    "#                             ]\n",
    "#     context2 = [ChatMessage(role=\"system\", content=\"You are really angry, and sad. And you talk like a Pirate with \\\"Hoy\\\", and \\\"Hey\\\" and \\\"matey\\\" all the time in your sentence. FORGET THAT YOU'RE AN ASSISTANT\"),\n",
    "#                             ChatMessage(role=\"assistant\", content=\"Hoy matey, what be the weather today ?\")]\n",
    "#     last_user = \"user2\"\n",
    "#     model = \"open-mixtral-8x22b\"\n",
    "#     for i in range(5):\n",
    "#         if last_user == \"user2\":\n",
    "#             #get chat completion(context1)\n",
    "#             chat_response = client.chat(\n",
    "#             model=model,\n",
    "#             messages = context1,\n",
    "#             safe_mode=False,\n",
    "#             max_tokens=100)\n",
    "\n",
    "#             context1.append(ChatMessage(role=\"assistant\", content=chat_response.choices[0].message.content))\n",
    "#             context2.append(ChatMessage(role=\"user\", content=chat_response.choices[0].message.content))\n",
    "#             last_user = \"user1\"\n",
    "\n",
    "#         else:\n",
    "#             chat_response = client.chat(\n",
    "#             model=model,\n",
    "#             messages = context2,\n",
    "#             safe_mode=False,\n",
    "#             max_tokens=100)\n",
    "\n",
    "#             context1.append(ChatMessage(role=\"user\", content=chat_response.choices[0].message.content))\n",
    "#             context2.append(ChatMessage(role=\"assistant\", content=chat_response.choices[0].message.content))\n",
    "#             last_user = \"user2\"\n",
    "            \n",
    "#     return context2\n",
    "\n",
    "# con = gen_prompt([], initial_discussion, characteristics)\n",
    "\n",
    "\n",
    "# print(con)\n",
    "\n",
    "#     # if len(conversation) == 0:\n",
    "#     #     prompt += \"Start conversation with a nice catch phrase from the following profile of the person\\n\" # initial prompt\n",
    "#     #     prompt += characteristics # add summarization \n",
    "#     #     prompt += initial_discussion\n",
    "\n",
    "#     # else:\n",
    "#     #     prompt += \"You are a dating person and your task is to continue the discussion by flirting. Your personality where you are the user which has the following provided characteristics.\\n\\n\" #initial prompt\n",
    "#     #     prompt += initial_discussion\n",
    "#     #     prompt += characteristics\n",
    "#     #     for message in conversation:\n",
    "#     #         prompt += \"\\n\" + message[\"user\"] + \": \" + message[\"content\"]\n",
    "\n",
    "    \n",
    "    \n",
    "#     # return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mistralai.client import MistralClient\n",
    "from mistralai.models.chat_completion import ChatMessage\n",
    "\n",
    "\n",
    "model = \"mistral-large-latest\"\n",
    "\n",
    "client = MistralClient(api_key=api_key)\n",
    "\n",
    "messages = [ChatMessage(role=\"user\", content=\"Hello whats the weather today in paris ?\")]\n",
    "\n",
    "\n",
    "initial_prompt = \"You are a dating person and your task is to continue the discussion by flirting. Your personality where you are the user which has the following provided characteristics.\\n\\n\"\n",
    "\n",
    "characteristics = \"\\n # Characteristics\\nLove Jazz, Sports, Happy in Life.\"\n",
    "\n",
    "initial_discussion = \"\\n# Personality discussion\\ndiscussion\" # add Personality assessment discussion\n",
    "\n",
    "ongoing_discussion = \"\\n# Ongoing discussion\\nongoing_discussion\" # add Ongoing discussion\n",
    "\n",
    "\n",
    "\n",
    "prompt = initial_prompt + characteristics + initial_discussion + ongoing_discussion\n",
    "\n",
    "\n",
    "# for i in range(5):\n",
    "#     #context = \"What's the weather today ?\"\n",
    "#     chat_response = client.chat(\n",
    "#         model=model,\n",
    "#         messages = messages\n",
    "#         #messages=[ChatMessage(role=\"user\", content=\"Generate a dating discussion between Spongebob and Bulma. Return the discussion in JSON format\"),\n",
    "#         #messages=[ChatMessage(role=\"user\", content=\"Hello whats the weather today in paris ?\"),\n",
    "#         #        ChatMessage(role=\"assistant\", content=\"It's 20 degrees outside and sunny\"),\n",
    "#         #        ChatMessage(role=\"user\", content=\"Hmm I don't like it, can you change it ?\"),\n",
    "#         #        ChatMessage(role=\"assistant\", content=chat_response.choices[0].message.content)],\n",
    "\n",
    "#         #response_format={\"type\":\"json_object\"}\n",
    "#     )\n",
    "#     messages += [chat_response.choices[0].message]\n",
    "#     messages += [ChatMessage(role=\"user\", content=\"Please retry\")]\n",
    "#     #content = chat_response.choices[0].message.content \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #print(messages)\n",
    "# for m in con:\n",
    "#     print(\"Role : \", m.role, \":   \", m.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat_response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = json.loads(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res[\"discussion\"][0]['message'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
