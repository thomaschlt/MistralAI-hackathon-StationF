{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv() # load the environment variables from the .env file\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_chat_prompt = \"What's your favorite way to spend a weekend?\\nWhat kind of music do you enjoy the most, and why?\\nDo you prefer staying in or going out on a typical night?\\nWhat's the best vacation you've ever been on?\\nWhat are your top three favorite movies?\\nDo you enjoy cooking, and if so, what's your signature dish?\\nWhat are some of your hobbies or interests outside of work?\\nAre you more of an introvert or an extrovert?\\nWhat's one book that has significantly influenced your life?\\nDo you have any pets? If so, tell me about them.\\nWhat is your dream job or career goal?\\nHow do you usually handle stress or difficult situations?\\nWhat's your family like? Are you close to them?\\nWhat qualities do you value most in a friend or partner?\\nWhat's your favorite childhood memory?\\nHow do you like to celebrate your birthday?\\nDo you enjoy any sports or physical activities?\\nWhat are some things on your bucket list?\\nHow do you define success and what does it look like for you?\\nWhat's the most spontaneous thing you've ever done?\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from openai import OpenAI\n",
    "import os\n",
    "client = OpenAI()\n",
    "model = \"gpt-3.5-turbo\"\n",
    "# Define a function to generate chat completion using the OpenAI API\n",
    "\n",
    "#TODO need to add prompt for fake AI init\n",
    "#TODO GROQ \n",
    "#TODO change to mistral (try)\n",
    "\n",
    "def init_prompt(conversation, initial_discussion, characteristics, prompt):\n",
    "    context1 = [\n",
    "        {\"role\": \"system\", \"content\": prompt + \" GIVE ONLY SHORT ANSWERS.\"},\n",
    "        {\"role\": \"user\", \"content\": \"How you doing little cutey?\"}\n",
    "    ]\n",
    "    context2 = [\n",
    "        #{\"role\": \"system\", \"content\": \"You are really angry, and sad. And you talk like a Pirate with \\\"Hoy\\\", and \\\"Hey\\\" and \\\"matey\\\" all the time in your sentence. GIVE ONLY SHORT ANSWERS.\"},\n",
    "        {\"role\": \"system\", \"content\": \"You are a really lovely and happy person that loves to know more about the people's lives. Your task is to ask whatever question you find is the most adapted to know someone in a few steps. You can get inspired by the ones provided below. GIVE ONLY SHORT ANSWERS.\\nQuestions :\\n\"+init_chat_prompt},\n",
    "        {\"role\": \"assistant\", \"content\": \"How you doing little cutey?\"}\n",
    "    ]\n",
    "    last_user = \"user2\"\n",
    "    #model = \"text-davinci-002\"  # Replace with your desired model\n",
    "\n",
    "    for _ in range(10):\n",
    "        if last_user == \"user2\":\n",
    "            chat_response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=context1,\n",
    "                #safe_mode=False\n",
    "            )\n",
    "            new_content = chat_response.choices[0].message.content\n",
    "            context1.append({\"role\": \"assistant\", \"content\": new_content})\n",
    "            context2.append({\"role\": \"user\", \"content\": new_content})\n",
    "            last_user = \"user1\"\n",
    "        else:\n",
    "            chat_response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=context2,\n",
    "                #safe_mode=False\n",
    "            )\n",
    "            new_content = chat_response.choices[0].message.content\n",
    "            context1.append({\"role\": \"user\", \"content\": new_content})\n",
    "            context2.append({\"role\": \"assistant\", \"content\": new_content})\n",
    "            last_user = \"user2\"\n",
    "\n",
    "    return context2\n",
    "\n",
    "# Example usage\n",
    "#con1, con2 = toy_prompt([], 'initial_discussion', 'characteristics')\n",
    "\n",
    "con = init_prompt([], 'initial_discussion', 'characteristics',\"You are a cute woman that loves jazz and dogs. Looking for a husband after losing hers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in con:\n",
    "    print(\"Role : \", m[\"role\"], \":   \", m[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con2 = init_prompt([], 'initial_discussion', 'characteristics',\"You are a geek that loves redbull but likes to go outside for a chill drink from time to time. Looking for nothing too serious as you're not confortable with sentiments.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in con2:\n",
    "    print(\"Role : \", m[\"role\"], \":   \", m[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract user personality\n",
    "\n",
    "def gen_prompt_from_llm_user_conversation(conversation):\n",
    "    #TODO phrases too short need to feed the conversation, not cut it \n",
    "    prompt = \"You're roleplaying a dating person. In the following you'll find details about the person. Match as best as you can the following writing style of the person's messages. GIVE RELATIVELY SHORT ANSWERS.\" #basic prompt\n",
    "\n",
    "    user_messages = \"\"\n",
    "    for message in conversation:\n",
    "        if message[\"role\"] == \"user\":\n",
    "            user_messages += \"\\n\" + message[\"content\"]\n",
    "\n",
    "    context = conversation\n",
    "    context += [{\"role\": \"system\", \"content\": \"You're a psychologist. From a past interaction with the user, summarise the important informations about him. GIVE ONLY THE INFORMATION.\" }]\n",
    "    chat_response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=context,\n",
    "            )\n",
    "    \n",
    "    \n",
    "\n",
    "    prompt += \"\\nDetails :\\n\" + chat_response.choices[0].message.content\n",
    "    prompt += \"\\nPerson's messages :\" + user_messages\n",
    "\n",
    "    #return the general charastics of the user + extracts from the conversation\n",
    "    return prompt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "conversation_1 = [ {\"role\": \"user\", \"content\": \"How are   you doing lovely\" }]\n",
    "conversation_2 = [ {\"role\": \"user\", \"content\": \"How are   youackanksdnal;sdjkalkdally\" }]\n",
    "\n",
    "\n",
    "init1 = gen_prompt_from_llm_user_conversation(con)\n",
    "init2 = gen_prompt_from_llm_user_conversation(con2)\n",
    "\n",
    "print(init1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(init2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_LLM_to_LLM_conversation(init1, init2):\n",
    "    context1 = []\n",
    "    #context1 += [\n",
    "    #    {\"role\": \"system\", \"content\": \"The past discussion as user defines yourself, continue the discussion with this personality. GIVE SHORT ANSWERS\"},\n",
    "    #    {\"role\": \"user\", \"content\": \"How are you doing lovely boy ?\"}\n",
    "    #]\n",
    "    context1 += [{\"role\": \"system\", \"content\": init1}] \n",
    "    \n",
    "    context = [{\"role\": \"system\", \"content\": \"What's the best catchphrase following the next instructions ? GIVE ONLY A SHORT ANSWER.\\n\"+init1}] #ice breaker prompt\n",
    "    chat_response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=context,\n",
    "    )\n",
    "    ice_breaker = chat_response.choices[0].message.content\n",
    "    context1 += [{\"role\": \"user\", \"content\": ice_breaker}]\n",
    "\n",
    "    context2 = [\n",
    "        {\"role\": \"system\", \"content\": init2},\n",
    "        {\"role\": \"assistant\", \"content\": ice_breaker}\n",
    "    ]\n",
    "    last_user = \"user2\"\n",
    "    #model = \"text-davinci-002\"  # Replace with your desired model\n",
    "\n",
    "    for _ in range(10):\n",
    "        if last_user == \"user2\":\n",
    "            chat_response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=context1,\n",
    "                #safe_mode=False\n",
    "            )\n",
    "            new_content = chat_response.choices[0].message.content\n",
    "            context1.append({\"role\": \"assistant\", \"content\": new_content})\n",
    "            context2.append({\"role\": \"user\", \"content\": new_content})\n",
    "            last_user = \"user1\"\n",
    "        else:\n",
    "            chat_response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=context2,\n",
    "                #safe_mode=False\n",
    "            )\n",
    "            new_content = chat_response.choices[0].message.content\n",
    "            context1.append({\"role\": \"user\", \"content\": new_content})\n",
    "            context2.append({\"role\": \"assistant\", \"content\": new_content})\n",
    "            last_user = \"user2\"\n",
    "\n",
    "    return context1,context2\n",
    "\n",
    "con11, con22 = generate_LLM_to_LLM_conversation(init1, init2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in con11:\n",
    "    print(\"Role : \", m[\"role\"], \":   \", m[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in con22:\n",
    "    print(\"Role : \", m[\"role\"], \":   \", m[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def judge_conversation():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from itertools import cycle\n",
    "\n",
    "# def gen_prompt(conversation, initial_discussion, characteristics):\n",
    "#     context1 = []\n",
    "#     context2 = []\n",
    "#     prompt = \"\"\n",
    "#     context1 = [ChatMessage(role=\"system\", content=\"You are really happy and energetic and talk like a CEO, talking about profit all the time. FORGET THAT YOU'RE AN ASSISTANT.\"),\n",
    "#                             ChatMessage(role=\"user\", content=\"Hoy matey, what be the weather today ?\"),\n",
    "#                             ]\n",
    "#     context2 = [ChatMessage(role=\"system\", content=\"You are really angry, and sad. And you talk like a Pirate with \\\"Hoy\\\", and \\\"Hey\\\" and \\\"matey\\\" all the time in your sentence. FORGET THAT YOU'RE AN ASSISTANT\"),\n",
    "#                             ChatMessage(role=\"assistant\", content=\"Hoy matey, what be the weather today ?\")]\n",
    "#     last_user = \"user2\"\n",
    "#     model = \"open-mixtral-8x22b\"\n",
    "#     for i in range(5):\n",
    "#         if last_user == \"user2\":\n",
    "#             #get chat completion(context1)\n",
    "#             chat_response = client.chat(\n",
    "#             model=model,\n",
    "#             messages = context1,\n",
    "#             safe_mode=False,\n",
    "#             max_tokens=100)\n",
    "\n",
    "#             context1.append(ChatMessage(role=\"assistant\", content=chat_response.choices[0].message.content))\n",
    "#             context2.append(ChatMessage(role=\"user\", content=chat_response.choices[0].message.content))\n",
    "#             last_user = \"user1\"\n",
    "\n",
    "#         else:\n",
    "#             chat_response = client.chat(\n",
    "#             model=model,\n",
    "#             messages = context2,\n",
    "#             safe_mode=False,\n",
    "#             max_tokens=100)\n",
    "\n",
    "#             context1.append(ChatMessage(role=\"user\", content=chat_response.choices[0].message.content))\n",
    "#             context2.append(ChatMessage(role=\"assistant\", content=chat_response.choices[0].message.content))\n",
    "#             last_user = \"user2\"\n",
    "            \n",
    "#     return context2\n",
    "\n",
    "# con = gen_prompt([], initial_discussion, characteristics)\n",
    "\n",
    "\n",
    "# print(con)\n",
    "\n",
    "#     # if len(conversation) == 0:\n",
    "#     #     prompt += \"Start conversation with a nice catch phrase from the following profile of the person\\n\" # initial prompt\n",
    "#     #     prompt += characteristics # add summarization \n",
    "#     #     prompt += initial_discussion\n",
    "\n",
    "#     # else:\n",
    "#     #     prompt += \"You are a dating person and your task is to continue the discussion by flirting. Your personality where you are the user which has the following provided characteristics.\\n\\n\" #initial prompt\n",
    "#     #     prompt += initial_discussion\n",
    "#     #     prompt += characteristics\n",
    "#     #     for message in conversation:\n",
    "#     #         prompt += \"\\n\" + message[\"user\"] + \": \" + message[\"content\"]\n",
    "\n",
    "    \n",
    "    \n",
    "#     # return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mistralai.client import MistralClient\n",
    "from mistralai.models.chat_completion import ChatMessage\n",
    "\n",
    "\n",
    "model = \"mistral-large-latest\"\n",
    "\n",
    "client = MistralClient(api_key=api_key)\n",
    "\n",
    "messages = [ChatMessage(role=\"user\", content=\"Hello whats the weather today in paris ?\")]\n",
    "\n",
    "\n",
    "initial_prompt = \"You are a dating person and your task is to continue the discussion by flirting. Your personality where you are the user which has the following provided characteristics.\\n\\n\"\n",
    "\n",
    "characteristics = \"\\n # Characteristics\\nLove Jazz, Sports, Happy in Life.\"\n",
    "\n",
    "initial_discussion = \"\\n# Personality discussion\\ndiscussion\" # add Personality assessment discussion\n",
    "\n",
    "ongoing_discussion = \"\\n# Ongoing discussion\\nongoing_discussion\" # add Ongoing discussion\n",
    "\n",
    "\n",
    "\n",
    "prompt = initial_prompt + characteristics + initial_discussion + ongoing_discussion\n",
    "\n",
    "\n",
    "# for i in range(5):\n",
    "#     #context = \"What's the weather today ?\"\n",
    "#     chat_response = client.chat(\n",
    "#         model=model,\n",
    "#         messages = messages\n",
    "#         #messages=[ChatMessage(role=\"user\", content=\"Generate a dating discussion between Spongebob and Bulma. Return the discussion in JSON format\"),\n",
    "#         #messages=[ChatMessage(role=\"user\", content=\"Hello whats the weather today in paris ?\"),\n",
    "#         #        ChatMessage(role=\"assistant\", content=\"It's 20 degrees outside and sunny\"),\n",
    "#         #        ChatMessage(role=\"user\", content=\"Hmm I don't like it, can you change it ?\"),\n",
    "#         #        ChatMessage(role=\"assistant\", content=chat_response.choices[0].message.content)],\n",
    "\n",
    "#         #response_format={\"type\":\"json_object\"}\n",
    "#     )\n",
    "#     messages += [chat_response.choices[0].message]\n",
    "#     messages += [ChatMessage(role=\"user\", content=\"Please retry\")]\n",
    "#     #content = chat_response.choices[0].message.content \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #print(messages)\n",
    "# for m in con:\n",
    "#     print(\"Role : \", m.role, \":   \", m.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat_response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = json.loads(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res[\"discussion\"][0]['message'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
